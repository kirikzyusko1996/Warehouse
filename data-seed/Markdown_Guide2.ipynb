{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kirillzyusko/Warehouse/blob/dev/Markdown_Guide2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTnBkRyvGvhW"
   },
   "source": [
    "Formatting text in Colaboratory: A guide to Colaboratory markdown\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "oRXnjvGTmwik",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(\"series-transformed.csv\")\n",
    "dataset = dataframe.values\n",
    "\n",
    "X = dataset[:, 2:77]\n",
    "y = dataframe['output']\n",
    "\n",
    "X_train, X_test = np.split(X, [int(.7 * len(X))])\n",
    "y_train, y_test = np.split(y, [int(.7 * len(y))])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "n_splits = int((len(X_train)-3) / 3)\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "print(n_splits)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "# fit scaler on training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "input_size = X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(int(input_size * 1.5), input_dim=input_size, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), activation='relu', name='hidden_1'))\n",
    "model.add(Dropout(0.4, name='dropout_1'))\n",
    "model.add(Dense(input_size, kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(0.01), name='hidden_2'))\n",
    "model.add(Dropout(0.4, name='dropout_2'))\n",
    "#model.add(Dense(int(input_size * 0.5), kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(0.01), name='hidden_3'))\n",
    "#model.add(Dropout(0.5, name='dropout_3'))\n",
    "model.add(Dense(1, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.01), name='output_layer'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = None\n",
    "start_time = time.time()\n",
    "for train_index, test_index in tscv.split(X_train):\n",
    "    # print(len(train_index))\n",
    "    Xx_train, Xx_test = X_train[train_index], X_train[test_index]\n",
    "    yy_train, yy_test = y_train[train_index], y_train[test_index]\n",
    "    history = model.fit(Xx_train, yy_train,\n",
    "              epochs=50,\n",
    "              verbose=0,\n",
    "              validation_data=(Xx_test, yy_test))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "loss_values = history.history['loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# 163418\n",
    "x_input = np.array(scaler.transform([[0,251651,330914,426493,328758,287274,398647,367155,320362,316503,283214,335058,327631,223894,306459,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0]]))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)\n",
    "\n",
    "x_input = np.array(scaler.transform(X))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "\n",
    "sum = 0\n",
    "index = []\n",
    "for i in range(len(yhat)):\n",
    "    percentage = (yhat[i][0]-y[i]) / yhat[i][0] * 100\n",
    "    # print(f'Predicted: {yhat[i][0]}, Real: {y[i]}, error: {yhat[i][0]-y[i]}, percentage: {percentage}')\n",
    "    sum += abs(percentage)\n",
    "    index.append(i)\n",
    "\n",
    "print(f'Average error: {sum / len(yhat)}')\n",
    "\n",
    "plt.plot(index, y, color='green')\n",
    "plt.plot(index, yhat, color='red', alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 194687\n",
    "# 203000.00 - 2000 epoch/normalized\n",
    "# 195594.27 - 20000 epoch/not normalized\n",
    "# 346240.53 - 20000 epoch/normalized\n",
    "# 195012.86 - 200000 epoch/normalized\n",
    "\n",
    "# 163418\n",
    "# 163348.62 - 55000 epochs/normalized\n",
    "# 162871.17 - 2000  epochs/normalized\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Markdown Guide",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
